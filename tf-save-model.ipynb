{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Servable ML Model with Tensorflow\n",
    "In this notebook we will create, save, load, and employ models with Tensorflow. We will work through how to structure code to create models that can be saved and used for inference in the cloud or at the edge with applications such as dashboards, games, anomaly detection, and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.0\n",
      "Numpy version: 1.18.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "print(\"Tensorflow version:\", tf.version.VERSION)\n",
    "print(\"Numpy version:\", np.version.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'models' # We will save model to this directory\n",
    "DATA_PATH = os.path.join('data', 'data.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 2 includes a [SavedModel](https://www.tensorflow.org/guide/saved_model \"Tensorflow SavedModel docs\") format that can be utilised for transfer learning and/or inference in applications. In order for a model to be saveable, it has to be of the type `tf.Module`. Keras models satisfy this criterium and are thus relatively simple to save. Before jumping into a specific example of a Keras model, we will, however, address the components of a Tensorflow SavedModel with a general example using pure Tensorflow.\n",
    "\n",
    "# General Example in Pure Tensorflow\n",
    "\n",
    "## An Example Model\n",
    "A Tensorflow SavedModel can be created from a `tf.Module`, so let us create a very simple example model using this object structure. The saved model will include any `tf.Modules`s, any methods with the `@tf.function` decorator, and any `tf.Variable`, but will not include any Python code or functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearScaler(tf.Module):\n",
    "    '''\n",
    "    LinearScaler is a very simple linear function that takes in a variable, multiplies it\n",
    "     by a weight and adds a bias before returning the result.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(LinearScaler, self).__init__()\n",
    "        self.bias = tf.Variable(1.)\n",
    "        self.weight = tf.Variable(2.)\n",
    "    \n",
    "    # Uncomment to set input signature\n",
    "    @tf.function#(input_signature=[tf.TensorSpec([], tf.float32)])\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        Linearly rescale y = x * weight + bias\n",
    "        :param x: The variable to be linearly scaled\n",
    "        :type x: tf.float32\n",
    "        :output dict: \"y\" = x * weight + bias\n",
    "        '''\n",
    "        return {\"y\" : x * self.weight + self.bias}\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n",
    "    def calibrate(self, weight, bias):\n",
    "        '''\n",
    "        Set the parameters of the linear function.\n",
    "        :param weight: Weight of the linear function\n",
    "        :type weight: tf.float32\n",
    "        :param bias: Bias of the linear function\n",
    "        :type bias: tf.float32\n",
    "        '''\n",
    "        self.weight.assign(weight)\n",
    "        self.bias.assign(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make sure that is works by evaluating it. Note that by evaluating the model, it is compiled into a graph - a step that is needed before we save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearScaler()\n",
    "assert model(tf.constant(4.))[\"y\"].numpy() == 9 #  2 * 4 + 1\n",
    "model.calibrate(weight=5, bias=2)\n",
    "assert model(tf.constant(4.))[\"y\"].numpy() == 22 # 5 * 4 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! Now we have a working model. \n",
    "\n",
    "## Save and Load\n",
    "Let us go right ahead and save our model using the `tf.saved_model.save` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\no_signatures\\assets\n"
     ]
    }
   ],
   "source": [
    "no_signatures_path = os.path.join(MODEL_DIR, 'no_signatures')\n",
    "tf.saved_model.save(model, no_signatures_path) # Save the function to a dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model also only takes a single line with the `tf.saved_model.load` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.saved_model.load(no_signatures_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done so far will work perfectly well in many cases but, in some cases, the loaded model might behave differently from our expectations. Let us look at some characteristics of the model as it is now.<br><br>\n",
    "Firstly, the parameters of the loaded model are the same as the original at the time we saved it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight in loaded model: 5.0\n",
      "Bias in loaded model: 2.0\n"
     ]
    }
   ],
   "source": [
    "assert model.weight.numpy() == loaded_model.weight.numpy()\n",
    "print(\"Weight in loaded model:\", loaded_model.weight.numpy())\n",
    "print(\"Bias in loaded model:\", loaded_model.bias.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, the weight and bias parameters to not attain the values of a newly initialised model but the values that we set just before saving.<br><br>\n",
    "The loaded model will also evaluate inputs like the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y': <tf.Tensor: shape=(), dtype=float32, numpy=112.0>}\n"
     ]
    }
   ],
   "source": [
    "assert model(tf.constant(4.))[\"y\"].numpy() == loaded_model(tf.constant(4.))[\"y\"].numpy()\n",
    "assert model(tf.constant(22.))[\"y\"].numpy() == loaded_model(tf.constant(22.))[\"y\"].numpy()\n",
    "print(loaded_model(tf.constant(22.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is where the simillarity ends. Our original model will happily evaluate a different kind of input. Here for instance a Tensor of several floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y': <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 7., 12., 17.], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "print(model(tf.constant([1., 2., 3.]))) # Passing a Tensor of floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when providing the same input to our loaded model, we are presented with a ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got a ValueError:\n",
      " Could not find matching function to call loaded from the SavedModel. Got:\n",
      "  Positional arguments (1 total):\n",
      "    * Tensor(\"x:0\", shape=(3,), dtype=float32)\n",
      "  Keyword arguments: {}\n",
      "\n",
      "Expected these arguments to match one of the following 1 option(s):\n",
      "\n",
      "Option 1:\n",
      "  Positional arguments (1 total):\n",
      "    * TensorSpec(shape=(), dtype=tf.float32, name='x')\n",
      "  Keyword arguments: {}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(loaded_model(tf.constant([1., 2., 3.]))) # Passing a Tensor of floats\n",
    "except ValueError as e:\n",
    "    print(\"We got a ValueError:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not that we did anything wrong. It is just that we did not pay much attention to the *input* of our model. When we evaluated the model before saving it we caused the class we defined to be compiled into a `tf.Graph` object. The Graph object must assume a fixed structure of the input, i.e. an input signature. When we pass a new input to our original model, the graph can get recompiled to fit that input, but our saved and loaded model represents a fixed Graph that can only accept the expected input and will throw a ValueError otherwise. In other words, we should specify the input signature that our model should expect. We did specify the input signature implicitly when evaluating the model the first time, but we might want to do it explicitly.\n",
    "\n",
    "## Save with an Input Signature\n",
    "There are two was to specify an input signature to a Tensorflow method. One way is to use the `@tf.function` decorator. The `@tf.function` decorator takes an input_signature as a kwarg, and we can pass it a list of `tf.TensorSpec`s to template the input. We actually already did this in the example model for the `.calibrate` method to show that we are expecting two constant tensors as input, but for the `__call__` method, the signature was commented out.<br>\n",
    "The second method is to explicitly compile the `tf.Graph` and, in the process, passing an input signature. We can do this by invoking the `.get_concrete_function` method on the `__call__` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_signature = LinearScaler() # Get a fresh instance of the function\n",
    "input_signature_array = tf.TensorSpec([None], tf.float32) # Note! Specifices a Tensor array of floats\n",
    "call = model_with_signature.__call__.get_concrete_function(input_signature_array) # Compile Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we save the compiled graph, we will pass this compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\with_signature\\assets\n"
     ]
    }
   ],
   "source": [
    "with_signature_path = os.path.join(MODEL_DIR, 'with_signature')\n",
    "tf.saved_model.save(model_with_signature, with_signature_path, signatures=call) # Save the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load up the model and whether it can handle the input we specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y': <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 7.,  9., 11.], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "loaded_model_with_signature = tf.saved_model.load(with_signature_path)\n",
    "print(loaded_model_with_signature(tf.constant([3., 4., 5.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is one challenge solved; we now know how to explicitly define the input signature. The model works with the specified input, but any other type of input will cause a ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got a ValueError: Could not find matching function to call loaded from the SavedModel. Got:\n",
      "  Positional arguments (1 total):\n",
      "    * Tensor(\"x:0\", shape=(), dtype=float32)\n",
      "  Keyword arguments: {}\n",
      "\n",
      "Expected these arguments to match one of the following 1 option(s):\n",
      "\n",
      "Option 1:\n",
      "  Positional arguments (1 total):\n",
      "    * TensorSpec(shape=(None,), dtype=tf.float32, name='x')\n",
      "  Keyword arguments: {}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(loaded_model_with_signature(tf.constant(3.)))\n",
    "except ValueError as e:\n",
    "    print(\"We got a ValueError:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save with Multiple Input Signatures\n",
    "We might want to save and serve our model with multiple kinds of inputs. For instance in our example model, we would like to serve the model to single inputs as well as a list of inputs. Everything else, like the weights and biases, should be the same, but the input should be flexible. One way to do this might be to save two or more seperate models, but that would create multiple duplicates of the same weights and, as a result, extra operational overhead. Fortunately, there is a better way.<br>\n",
    "A loaded model actually has a dictionary, `.signatures`, mapping the input signatures that can be evaluated in the model. We only specified a single input signature, so there should be only one entry in the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['serving_default']\n"
     ]
    }
   ],
   "source": [
    "print(list(loaded_model_with_signature.signatures.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is indeed only a single entry, which is the default `\"serving_default\"` entry. When we call the loaded model and do not specify a key to this dictionary, it will look up `\"serving_default\"` and decide whether the input signature matches the input we used to call the model.<br>\n",
    "We could also have supplied the key and created an object of our model expecting the specified input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y': <tf.Tensor: shape=(), dtype=float32, numpy=13.0>}\n"
     ]
    }
   ],
   "source": [
    "inference_object_array = loaded_model_with_signature.signatures[\"serving_default\"]\n",
    "print(inference_object_array(tf.constant(6.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this functionality to create one saved model that serves multiple kinds of inputs. We do this by supplying a dictionary mapping keys to Graphs to the `tf.saved_model.save` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\multiple_signatures\\assets\n"
     ]
    }
   ],
   "source": [
    "model_multiple_signatures = LinearScaler() # Get a fresh instance of the function\n",
    "\n",
    "# Input signatures\n",
    "input_signature_single = tf.TensorSpec(None, tf.float32) # Specifices a single Tensor float\n",
    "input_signature_array = tf.TensorSpec([None], tf.float32) # Specifices a Tensor array of floats\n",
    "\n",
    "# Compiled Graphs\n",
    "call_single = model_multiple_signatures.__call__.get_concrete_function(input_signature_single)\n",
    "call_array = model_multiple_signatures.__call__.get_concrete_function(input_signature_array)\n",
    "\n",
    "# Input signature dictionary\n",
    "signatures = {\"serving_default\": call_single,\n",
    "              \"array_input\": call_array}\n",
    "\n",
    "# Save the model\n",
    "multiple_signatures_path = os.path.join(MODEL_DIR, \"multiple_signatures\")\n",
    "tf.saved_model.save(model_multiple_signatures, multiple_signatures_path, signatures=signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load up the model and create two different inference objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature dictionary:\n",
      "_SignatureMap({'serving_default': <tensorflow.python.saved_model.load._WrapperFunction object at 0x0000018D5CE59E48>, 'array_input': <tensorflow.python.saved_model.load._WrapperFunction object at 0x0000018D5CE24FC8>})\n"
     ]
    }
   ],
   "source": [
    "loaded_model_multiple_signatures = tf.saved_model.load(multiple_signatures_path)\n",
    "print(\"Signature dictionary:\")\n",
    "print(loaded_model_multiple_signatures.signatures)\n",
    "inference_object_single = loaded_model_multiple_signatures.signatures[\"serving_default\"]\n",
    "inference_object_array = loaded_model_multiple_signatures.signatures[\"array_input\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference objects still point to the same model, so if we change the parameters of the model, it will apply to both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y': <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 7.,  9., 11.], dtype=float32)>}\n",
      "{'y': <tf.Tensor: shape=(), dtype=float32, numpy=11.0>}\n",
      "{'y': <tf.Tensor: shape=(3,), dtype=float32, numpy=array([13., 16., 19.], dtype=float32)>}\n",
      "{'y': <tf.Tensor: shape=(), dtype=float32, numpy=19.0>}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate two types of inputs\n",
    "print(inference_object_array(tf.constant([3., 4., 5.])))\n",
    "print(inference_object_single(tf.constant(5.)))\n",
    "\n",
    "# Change parameters of the model\n",
    "loaded_model_multiple_signatures.calibrate(tf.constant(3.), tf.constant(4.))\n",
    "\n",
    "# Evaluate the same input again - note the difference in both!\n",
    "print(inference_object_array(tf.constant([3., 4., 5.])))\n",
    "print(inference_object_single(tf.constant(5.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this also means that our model is mutable even during inference. In fact the SavedModel format can be used for inference as well as retraining or transfer learning cases.<br>\n",
    "Now we know almost everythin there is to know about the SavedModel format. Let's just take a quick look inside one of the directories created each time we save a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inside a Saved Model\n",
    "Let us have a look at what is in one of the directories we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assets', 'saved_model.pb', 'variables']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(multiple_signatures_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved model consists of several elements.<br>\n",
    "- `saved_model.pb` contains the model architecture that is used to rebuild the function\n",
    "- The directory `variables` contain one or more data files holding the values of the parameters in the model at the time it was saved. For large models with billions of parameters, these data files can grow large. A `variables.index` file maps the stored parameters to their right spot in the function\n",
    "- The directory `assets` holds additional artefacts needed to recreate the function, but should be empty in our case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Data Science Workflow Example\n",
    "Now that we are clear on the basics, let us take a look at a more realistic workflow. Defining, saving, and serving a model built with Keras.<br>\n",
    "Our model starts with a bit of data and something to be modelled. I have prepared a small dataset consisting of weather observations from a station that observes temperature, relative humidity, air pressure, and whether it is raining or not. Our task is to build a model that predicts whether it rains or not given the temperature, relative humidity, and air pressure. Our target is not to build an awesome or precise model. It is to mimic a real data model lifecycle.<br>\n",
    "## Example Data\n",
    "First order of business, let us have a look at the example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example observations\n",
      "      pressure  rain  temperature  humidity\n",
      "2478    1007.6     0          2.7      36.0\n",
      "1174     994.3     0          3.1      77.0\n",
      "1474     984.2     0          5.0      85.0\n",
      "1655     999.7     0          4.3      95.0\n",
      "1393     997.7     0         -3.0      74.0\n",
      "684      984.8     1          1.9      95.0\n",
      "1449     974.8     1          0.7      95.0\n",
      "1139     978.1     0          6.8      77.0\n",
      "1904    1026.8     0          1.6      57.0\n",
      "1762    1018.6     0          0.0      50.0\n",
      "\n",
      "Statistics\n",
      "          pressure  temperature     humidity\n",
      "count  3151.000000  3151.000000  3151.000000\n",
      "mean   1005.235989     4.737417    71.054268\n",
      "std      15.113994     3.998234    20.464912\n",
      "min     951.100000    -6.400000    22.000000\n",
      "25%     996.150000     2.200000    55.000000\n",
      "50%    1005.700000     5.000000    75.000000\n",
      "75%    1016.500000     6.700000    89.000000\n",
      "max    1036.400000    20.100000    97.000000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "feature_cols = ['pressure', 'temperature', 'humidity']\n",
    "label_col = ['rain']\n",
    "print(\"Example observations\")\n",
    "print(df.sample(10))\n",
    "print()\n",
    "print(\"Statistics\")\n",
    "feature_stats = df[feature_cols].describe()\n",
    "print(feature_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the Data\n",
    "The data is of relatively high quality, all we need to do is to reduce the numerical difference and align the variances. Specifically, we will standardise (z norm) the features. If we were just doing analytics this is rather straightforward and we could use all sorts of out of the box functions. But later on we will be serving the model given new data, and any transformations we apply now we should be able to apply to production data later. Fortunately, we know how to build and save a Tensorflow model, so let us build standardisation as as a `tf.Module` with the means and standard deviations of our features as `tf.Variable`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZNorm(tf.Module):\n",
    "    '''\n",
    "    Implements standardisation (Z normalisation) as a Tensorflow model.\n",
    "    Set the means and standard deviations of the data using the calibrate mathod\n",
    "     before calling the model on data. The model will not change the data if the\n",
    "     means and standard deviations are not set.\n",
    "    :param num_features: Expected number of features\n",
    "    :type num_features: int\n",
    "    '''\n",
    "    def __init__(self, num_features):\n",
    "        super(ZNorm, self).__init__()\n",
    "        self.std_devs = tf.Variable(tf.ones([num_features]), dtype=tf.float32)\n",
    "        self.means = tf.Variable(tf.zeros([num_features]), dtype=tf.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        Compute the Z norm of input features\n",
    "        :param x: Tensor of length num_features\n",
    "        :type x: Tensor of floats\n",
    "        :output: x' = (x - mean)/std_dev\n",
    "        '''\n",
    "        return {\"x_prime\" : tf.divide(tf.subtract(x, self.means), self.std_devs)}\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None], tf.float32), tf.TensorSpec([None], tf.float32)])\n",
    "    def calibrate(self, means, standard_deviations):\n",
    "        '''\n",
    "        Sets the means and standard deviations of the standardiser.\n",
    "        :param means: Means of the features in the same order as the features\n",
    "        :type means: list\n",
    "        :param standard_deviations: Standard deviations of the features in the same order as the features\n",
    "        :type standard_deviations: list\n",
    "        '''\n",
    "        self.std_devs.assign(standard_deviations)\n",
    "        self.means.assign(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a Tensorflow model that can take our weather data as input and output standardised features, provided we have set the proper means and standard deviations to do so. So let us create an instance of the model, set the parameters, and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\standardiser\\assets\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "num_features = 3\n",
    "standardiser = ZNorm(num_features) # Get our transformer model\n",
    "means = feature_stats.loc['mean',:].values\n",
    "std_devs = feature_stats.loc['std',:].values\n",
    "standardiser.calibrate(means=means, standard_deviations=std_devs) # Set the parameters of the transformer model\n",
    "# Specify input signatures\n",
    "input_signature_array = tf.TensorSpec([num_features], tf.float32) # Specifices a Tensor array of entries\n",
    "input_signature_single = tf.TensorSpec(num_features, tf.float32) # Specifices a single Tensor entry\n",
    "call_array = standardiser.__call__.get_concrete_function(input_signature_array) # Compile Graph\n",
    "call_single = standardiser.__call__.get_concrete_function(input_signature_single) # Compile Graph\n",
    "signatures = {\"serving_default\": call_array,\n",
    "              \"single_input\": call_single}\n",
    "# Save the model\n",
    "standardiser_path = os.path.join(MODEL_DIR, 'standardiser')\n",
    "tf.saved_model.save(standardiser, standardiser_path, signatures=signatures) # Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could continue to use the model we configured, but for the purpose of demonstration and to make sure that it works, let us load the model and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_standardiser = tf.saved_model.load(standardiser_path)\n",
    "inference_standardiser = loaded_standardiser.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the transformer model to produce features from our raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise(pandas_row):\n",
    "    '''\n",
    "    Convenience function to apply our Tensorflow functions to a Dataframe\n",
    "    '''\n",
    "    x = tf.constant(pandas_row.values, dtype=tf.float32)\n",
    "    xp = inference_standardiser(x)[\"x_prime\"].numpy()\n",
    "    return pd.Series(xp.tolist())\n",
    "features = df[feature_cols].apply(standardise, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to see whether we did everything right. Remember, we expect our features to be centered around 0 with unit standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3151.000000</td>\n",
       "      <td>3.151000e+03</td>\n",
       "      <td>3.151000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>-1.540569e-08</td>\n",
       "      <td>2.586615e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.581845</td>\n",
       "      <td>-2.785584e+00</td>\n",
       "      <td>-2.396994e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.601163</td>\n",
       "      <td>-6.346343e-01</td>\n",
       "      <td>-7.844777e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.030703</td>\n",
       "      <td>6.567480e-02</td>\n",
       "      <td>1.928047e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.745272</td>\n",
       "      <td>4.908624e-01</td>\n",
       "      <td>8.769025e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.061934</td>\n",
       "      <td>3.842342e+00</td>\n",
       "      <td>1.267815e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2\n",
       "count  3151.000000  3.151000e+03  3.151000e+03\n",
       "mean      0.000002 -1.540569e-08  2.586615e-08\n",
       "std       1.000000  1.000000e+00  1.000000e+00\n",
       "min      -3.581845 -2.785584e+00 -2.396994e+00\n",
       "25%      -0.601163 -6.346343e-01 -7.844777e-01\n",
       "50%       0.030703  6.567480e-02  1.928047e-01\n",
       "75%       0.745272  4.908624e-01  8.769025e-01\n",
       "max       2.061934  3.842342e+00  1.267815e+00"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything checks out, so we are ready to build and train a model. \n",
    "## Build and Train the Keras Model\n",
    "Our model will be a simple dense neural network built as a Keras sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymodel(X,y):\n",
    "    '''\n",
    "    A simple Keras nerual network with training and testing\n",
    "    :param X: The features (observations X features)\n",
    "    :type X: NumPy array\n",
    "    :param y: Labels\n",
    "    :type y: NumPy array\n",
    "    :output: Keras model\n",
    "    '''\n",
    "    seed = 42\n",
    "    \n",
    "    tf.random.set_seed(seed=seed)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed, shuffle=True)\n",
    "    \n",
    "    # Create Keras model\n",
    "    model = keras.Sequential(name=\"mymodel\", layers=[\n",
    "        keras.layers.InputLayer(input_shape=(X.shape[-1]), name=\"input\"),\n",
    "        keras.layers.Dense(6, activation=\"sigmoid\", name=\"dense\"),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\", name=\"y\")\n",
    "    ])\n",
    "\n",
    "    # Compile model with optimizer\n",
    "    model.compile(optimizer=keras.optimizers.Adam(0.05),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train model\n",
    "    model.fit(x=[X_train], y=[y_train], batch_size=50, epochs=5)\n",
    "\n",
    "    # Test model\n",
    "    test_loss, test_acc = model.evaluate(x=[X_test], y=[y_test])\n",
    "    print(\"Model test accuracy: \")\n",
    "    print(test_acc)\n",
    "    print(\"Baseline accuracy by random guessing:\")\n",
    "    print((np.random.randint(0,2,y_test.shape) == y_test).sum()/y_test.shape[0])\n",
    "    print(\"Baseline accuracy by guessing all zeros:\")\n",
    "    print(1 - np.mean(y_test))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is finally time to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2111 samples\n",
      "Epoch 1/5\n",
      "2111/2111 [==============================] - 1s 426us/sample - loss: 0.4073 - accuracy: 0.7963\n",
      "Epoch 2/5\n",
      "2111/2111 [==============================] - 0s 44us/sample - loss: 0.2505 - accuracy: 0.8802\n",
      "Epoch 3/5\n",
      "2111/2111 [==============================] - 0s 46us/sample - loss: 0.2320 - accuracy: 0.8835\n",
      "Epoch 4/5\n",
      "2111/2111 [==============================] - 0s 46us/sample - loss: 0.2206 - accuracy: 0.9000\n",
      "Epoch 5/5\n",
      "2111/2111 [==============================] - 0s 44us/sample - loss: 0.2176 - accuracy: 0.8972\n",
      "1040/1040 [==============================] - 0s 154us/sample - loss: 0.2257 - accuracy: 0.8990\n",
      "Model test accuracy: \n",
      "0.89903843\n",
      "Baseline accuracy by random guessing:\n",
      "0.49423076923076925\n",
      "Baseline accuracy by guessing all zeros:\n",
      "0.8692307692307693\n"
     ]
    }
   ],
   "source": [
    "X = features.values.astype('float32')\n",
    "y = df[label_col].values\n",
    "keras_model = mymodel(X, y) # Train and evaluate a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not beating the baseline by much, but let us assume that this is the model we wish to deploy.\n",
    "\n",
    "## Save and Load the Keras Model\n",
    "A Keras model has a `.save` method, but the low level Tensorflow method we have been working with, actually works just as well.<br>\n",
    "Our Keras model differs from our pure Tensorflow models in that we actually did specify an input signature. We did so by using `keras.InputLayer`, telling our model to expect inputs of shape num_observations X num_features. So let us go ahead and just save the model and have a look at what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\keras_model\\assets\n"
     ]
    }
   ],
   "source": [
    "keras_model_path = os.path.join(MODEL_DIR, 'keras_model')\n",
    "tf.saved_model.save(keras_model, keras_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load it up again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature dictionary:\n",
      "_SignatureMap({'serving_default': <tensorflow.python.saved_model.load._WrapperFunction object at 0x0000018D5E8C7EC8>})\n"
     ]
    }
   ],
   "source": [
    "loaded_keras_model = tf.saved_model.load(keras_model_path)\n",
    "print(\"Signature dictionary:\")\n",
    "print(loaded_keras_model.signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved model just has the default `serving_default` signature, which in this case specifies the signature we provided to the InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.20116203]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inference_model = loaded_keras_model.signatures['serving_default']\n",
    "print(inference_model(tf.constant([[0.55, -1.2, 1.4]]))['y']) # Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Finally, let us have a look at how inference could look. We might have a piece of weather data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are different ways we might get our data\n",
    "#inference_data = [[1005., 21., 78.],[1015., 10., 80.]]\n",
    "inference_data = [[1005., 21., 78.]]\n",
    "#inference_data = [1005., 21., 78.] # This will not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would convert that to a tensor and pass it through the standardiser to get our standardised features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardised features: tf.Tensor([[-0.01561215  4.067441    0.3393971 ]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inference_data_tensor = tf.constant(inference_data)\n",
    "inference_feature_tensor = inference_standardiser(tf.constant(inference_data_tensor))['x_prime']\n",
    "print(\"Standardised features:\", inference_feature_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass that tensor right on to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw prediction: [[0.1579284]]\n"
     ]
    }
   ],
   "source": [
    "raw_prediction = inference_model(inference_feature_tensor)['y'].numpy()\n",
    "print(\"Raw prediction:\", raw_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that remains is to interpret the prediction and get it to the format we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final prediction: [[0]]\n"
     ]
    }
   ],
   "source": [
    "classification_threshold = 0.5\n",
    "prediction = (raw_prediction >= classification_threshold).astype(int).tolist()\n",
    "print(\"Our final prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Production\n",
    "## Applications\n",
    "Greengrass, [Tensorflow serving](https://www.tensorflow.org/tfx/serving/serving_basic)\n",
    "## Managing Artefacts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
