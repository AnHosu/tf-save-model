{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Servable ML Model with Tensorflow\n",
    "In this notebook we will create, save, load, and employ models with Tensorflow. We will work through how to structure code to create models that can be saved and used for inference in the cloud or at the edge with applications such as dashboards, games, anomaly detection, and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.0\n",
      "Numpy version: 1.18.1\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "print(\"Tensorflow version:\", tf.version.VERSION)\n",
    "print(\"Numpy version:\", np.version.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'models' # We will save model to this directory\n",
    "DATA_PATH = os.path.join('data', 'data.csv') "
=======
    "df = pd.read_csv('data.csv')\n",
    "X = df[['pressure', 'temperature', 'humidity']].values\n",
    "y = df[['rain']].values"
>>>>>>> 12d57d7210b8d7386f1458780ec23abe1eb20947
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 2 includes a [SavedModel](https://www.tensorflow.org/guide/saved_model \"Tensorflow SavedModel docs\") format that can be utilised for transfer learning and/or inference in applications. In order for a model to be saveable, it has to be of the type `tf.Module`. Keras models satisfy this criterium and are thus relatively simple to save. Before jumping into a specific example of a Keras model, we will, however, address the components of a Tensorflow SavedModel with a general example using pure Tensorflow.\n",
    "\n",
    "# General Example in Pure Tensorflow\n",
    "\n",
    "## An Example Model\n",
    "A Tensorflow SavedModel can be created from a `tf.Module`, so let us create a very simple example model using this object structure. The saved model will include any `tf.Modules`s, any methods with the `@tf.function` decorator, and any `tf.Variable`, but will not include any Python code or functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearScaler(tf.Module):\n",
    "    '''\n",
    "    LinearScaler is a very simple linear function that takes in a variable, multiplies it\n",
    "     by a weight and adds a bias before returning the result.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(LinearScaler, self).__init__()\n",
    "        self.bias = tf.Variable(1.)\n",
    "        self.weight = tf.Variable(2.)\n",
    "    \n",
<<<<<<< HEAD
    "    # Uncomment to set input signature\n",
    "    @tf.function#(input_signature=[tf.TensorSpec([], tf.float32)])\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        Linearly rescale y = x * weight + bias\n",
    "        :param x: The variable to be linearly scaled\n",
    "        :type x: tf.float32\n",
    "        :output dict: \"y\" = x * weight + bias\n",
    "        '''\n",
    "        return {\"y\" : x * self.weight + self.bias}\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n",
    "    def calibrate(self, weight, bias):\n",
    "        '''\n",
    "        Set the parameters of the linear function.\n",
    "        :param weight: Weight of the linear function\n",
    "        :type weight: tf.float32\n",
    "        :param bias: Bias of the linear function\n",
    "        :type bias: tf.float32\n",
    "        '''\n",
    "        self.weight.assign(weight)\n",
    "        self.bias.assign(bias)\n",
=======
    "    # Create Keras model\n",
    "    model = keras.Sequential(name=\"mymodel\", layers=[\n",
    "        keras.layers.InputLayer(input_shape=(3), name=\"input\"),\n",
    "        keras.layers.Dense(3, activation=\"relu\", name=\"dense\"),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")\n",
    "    ])\n",
    "\n",
    "    print(\"Model architecture\")\n",
    "    model.summary()\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training model\")\n",
    "    model.fit(x=[X_train], y=[y_train], batch_size=100, epochs=10)\n",
    "\n",
    "    # Test model\n",
    "    print(\"Testing model\")\n",
    "    test_loss, test_acc = model.evaluate(x=[X_test], y=[y_test], verbose=2)\n",
    "\n",
    "    print(\"Test accuracy: \", test_acc)\n",
    "\n",
    "    # Predict\n",
    "    #predictions = model.predict(X_test)\n",
    "    \n",
    "    \n",
>>>>>>> 12d57d7210b8d7386f1458780ec23abe1eb20947
    "\n",
    "model = LinearScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Let us make sure that is works by evaluating it. Note that by evaluating the model, it is compiled into a graph - a step that is needed before we save"
=======
    "model = mymodel(X, y)\n",
    "\n",
    "# Make SavedModel\n",
    "keras_model_path = os.path.join(MODEL_DIR, 'keras_model')\n",
    "tf.saved_model.save(model, keras_model_path)"
>>>>>>> 12d57d7210b8d7386f1458780ec23abe1eb20947
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "assert model(tf.constant(4.))[\"y\"].numpy() == 9 #  2 * 4 + 1\n",
    "model.calibrate(weight=5, bias=2)\n",
    "assert model(tf.constant(4.))[\"y\"].numpy() == 22 # 5 * 4 + 2"
=======
    "imported = tf.saved_model.load('models')"
>>>>>>> 12d57d7210b8d7386f1458780ec23abe1eb20947
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! Now we have a working model. \n",
    "\n",
    "## Save and Load\n",
    "Let us go right ahead and save our model using the `tf.saved_model.save` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\no_signatures\\assets\n"
     ]
    }
   ],
   "source": [
    "no_signatures_path = os.path.join(MODEL_DIR, 'no_signatures')\n",
    "#model = LinearScaler() # Get a fresh model\n",
    "#model(tf.constant(4.))\n",
    "tf.saved_model.save(model, no_signatures_path) # Save the function to a dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model also only takes a single line with the `tf.saved_model.load` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.saved_model.load(no_signatures_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done so far will work perfectly well in many cases but, in some cases, the loaded model might behave differently from our expectations. Let us look at some characteristics of the model as it is now.<br><br>\n",
    "Firstly, the parameters of the loaded model are the same as the original at the time we saved it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight in loaded model: 5.0\n",
      "Bias in loaded model: 2.0\n"
     ]
    }
   ],
   "source": [
    "assert model.weight.numpy() == loaded_model.weight.numpy()\n",
    "print(\"Weight in loaded model:\", loaded_model.weight.numpy())\n",
    "print(\"Bias in loaded model:\", loaded_model.bias.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, the weight and bias parameters to not attain the values of a newly initialised model but the values that we set just before saving.<br><br>\n",
    "The loaded model will also evaluate inputs like the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "tf.Tensor(112.0, shape=(), dtype=float32)\n"
=======
      "Tensorflow version: 2.0.0\n",
      "Numpy version: 1.16.5\n"
>>>>>>> 12d57d7210b8d7386f1458780ec23abe1eb20947
     ]
    }
   ],
   "source": [
    "assert model(tf.constant(4.))[\"y\"].numpy() == loaded_model(tf.constant(4.)).numpy()\n",
    "assert model(tf.constant(22.))[\"y\"].numpy() == loaded_model(tf.constant(22.)).numpy()\n",
    "print(loaded_model(tf.constant(22.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is where the simillarity ends. Our original model will happily evaluate a different kind of input. Here for instance a Tensor of several floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y': <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 7., 12., 17.], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "print(model(tf.constant([1., 2., 3.]))) # Passing a Tensor of floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when providing the same input to our loaded model, we are presented with a ValueError"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
=======
   "execution_count": 3,
>>>>>>> 12d57d7210b8d7386f1458780ec23abe1eb20947
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got a ValueError:\n",
      " Could not find matching function to call loaded from the SavedModel. Got:\n",
      "  Positional arguments (1 total):\n",
      "    * Tensor(\"x:0\", shape=(3,), dtype=float32)\n",
      "  Keyword arguments: {}\n",
      "\n",
      "Expected these arguments to match one of the following 1 option(s):\n",
      "\n",
      "Option 1:\n",
      "  Positional arguments (1 total):\n",
      "    * TensorSpec(shape=(), dtype=tf.float32, name='x')\n",
      "  Keyword arguments: {}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(loaded_model(tf.constant([1., 2., 3.]))) # Passing a Tensor of floats\n",
    "except ValueError as e:\n",
    "    print(\"We got a ValueError:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not that we did anything wrong. It is just that we did not pay much attention to the *input* of our model. When we evaluated the model before saving it we caused the class we defined to be compiled into a `tf.Graph` object. The Graph object must assume a fixed structure of the input, i.e. an input signature. When we pass a new input to our original model, the graph can get recompiled to fit that input, but our saved and loaded model represents a fixed Graph that can only accept the expected input and will throw a ValueError otherwise. In other words, we should specify the input signature that our model should expect. We did specify the input signature implicitly when evaluating the model the first time, but we might want to do it explicitly.\n",
    "\n",
    "## Save with an Input Signature\n",
    "There are two was to specify an input signature to a Tensorflow method. One way is to use the `@tf.function` decorator. The `@tf.function` decorator takes an input_signature as a kwarg, and we can pass it a list of `tf.TensorSpec`s to template the input. We actually already did this in the example model for the `.calibrate` method to show that we are expecting two constant tensors as input, but for the `__call__` method, the signature was commented out.<br>\n",
    "The second method is to explicitly compile the `tf.Graph` and, in the process, passing an input signature. We can do this by invoking the `.get_concrete_function` method on the `__call__` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_signature = LinearScaler() # Get a fresh instance of the function\n",
    "input_signature_array = tf.TensorSpec([None], tf.float32) # Note! Specifices a Tensor array of floats\n",
    "call = model_with_signature.__call__.get_concrete_function(input_signature_array) # Compile Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we save the compiled graph, we will pass this compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "INFO:tensorflow:Assets written to: models\\with_signature\\assets\n"
=======
      "WARNING:tensorflow:From C:\\Users\\AEQN\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models\\no_signatures\\assets\n"
>>>>>>> 12d57d7210b8d7386f1458780ec23abe1eb20947
     ]
    }
   ],
   "source": [
    "with_signature_path = os.path.join(MODEL_DIR, 'with_signature')\n",
    "tf.saved_model.save(model_with_signature, with_signature_path, signatures=call) # Save the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load up the model and whether it can handle the input we specified"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7.  9. 11.], shape=(3,), dtype=float32)\n"
     ]
=======
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assets', 'saved_model.pb', 'variables']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 12d57d7210b8d7386f1458780ec23abe1eb20947
    }
   ],
   "source": [
    "loaded_model_with_signature = tf.saved_model.load(with_signature_path)\n",
    "print(loaded_model_with_signature(tf.constant([3., 4., 5.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is one challenge solved; we now know how to explicitly define the input signature. The model works with the specified input, but any other type of input will cause a ValueError"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 63,
=======
   "execution_count": 7,
>>>>>>> 12d57d7210b8d7386f1458780ec23abe1eb20947
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got a ValueError: Could not find matching function to call loaded from the SavedModel. Got:\n",
      "  Positional arguments (1 total):\n",
      "    * Tensor(\"x:0\", shape=(), dtype=float32)\n",
      "  Keyword arguments: {}\n",
      "\n",
      "Expected these arguments to match one of the following 1 option(s):\n",
      "\n",
      "Option 1:\n",
      "  Positional arguments (1 total):\n",
      "    * TensorSpec(shape=(None,), dtype=tf.float32, name='x')\n",
      "  Keyword arguments: {}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(loaded_model_with_signature(tf.constant(3.)))\n",
    "except ValueError as e:\n",
    "    print(\"We got a ValueError:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save with Multiple Input Signatures\n",
    "We might want to save and serve our model with multiple kinds of inputs. For instance in our example model, we would like to serve the model to single inputs as well as a list of inputs. Everything else, like the weights and biases, should be the same, but the input should be flexible. One way to do this might be to save two or more seperate models, but that would create multiple duplicates of the same weights and, as a result, extra operational overhead. Fortunately, there is a better way.<br>\n",
    "A loaded model actually has a dictionary, `.signatures`, mapping the input signatures that can be evaluated in the model. We only specified a single input signature, so there should be only one entry in the dictionary:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 66,
=======
   "execution_count": 8,
>>>>>>> 12d57d7210b8d7386f1458780ec23abe1eb20947
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['serving_default']\n"
     ]
    }
   ],
   "source": [
    "print(list(loaded_model_with_signature.signatures.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is indeed only a single entry, which is the default `\"serving_default\"` entry. When we call the loaded model and do not specify a key to this dictionary, it will look up `\"serving_default\"` and decide whether the input signature matches the input we used to call the model.<br>\n",
    "We could also have supplied the key and created an object of our model expecting the specified input:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_0': <tf.Tensor: shape=(), dtype=float32, numpy=13.0>}\n"
     ]
    }
   ],
   "source": [
    "inference_object_array = loaded_model_with_signature.signatures[\"serving_default\"]\n",
    "print(inference_object_array(tf.constant(6.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this functionality to create one saved model that serves multiple kinds of inputs. We do this by supplying a dictionary mapping keys to Graphs to the `tf.saved_model.save` method"
=======
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=202, shape=(), dtype=float32, numpy=22.0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model(4.)"
>>>>>>> 12d57d7210b8d7386f1458780ec23abe1eb20947
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\multiple_signatures\\assets\n"
     ]
    }
   ],
   "source": [
    "model_multiple_signatures = LinearScaler() # Get a fresh instance of the function\n",
    "\n",
    "# Input signatures\n",
    "input_signature_single = tf.TensorSpec(None, tf.float32) # Specifices a single Tensor float\n",
    "input_signature_array = tf.TensorSpec([None], tf.float32) # Specifices a Tensor array of floats\n",
    "\n",
    "# Compiled Graphs\n",
    "call_single = model_multiple_signatures.__call__.get_concrete_function(input_signature_single)\n",
    "call_array = model_multiple_signatures.__call__.get_concrete_function(input_signature_array)\n",
    "\n",
    "# Input signature dictionary\n",
    "signatures = {\"serving_default\": call_single,\n",
    "              \"array_input\": call_array}\n",
    "\n",
    "# Save the model\n",
    "multiple_signatures_path = os.path.join(MODEL_DIR, \"multiple_signatures\")\n",
    "tf.saved_model.save(model_multiple_signatures, multiple_signatures_path, signatures=signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load up the model and create two different inference objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_multiple_signatures = tf.saved_model.load(multiple_signatures_path)\n",
    "inference_object_single = loaded_model_multiple_signatures.signatures[\"serving_default\"]\n",
    "inference_object_array = loaded_model_multiple_signatures.signatures[\"array_input\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference objects still point to the same model, so if we change the parameters of the model, it will apply to both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y': <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 7.,  9., 11.], dtype=float32)>}\n",
      "{'y': <tf.Tensor: shape=(), dtype=float32, numpy=11.0>}\n",
      "{'y': <tf.Tensor: shape=(3,), dtype=float32, numpy=array([13., 16., 19.], dtype=float32)>}\n",
      "{'y': <tf.Tensor: shape=(), dtype=float32, numpy=19.0>}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate two types of inputs\n",
    "print(inference_object_array(tf.constant([3., 4., 5.])))\n",
    "print(inference_object_single(tf.constant(5.)))\n",
    "\n",
    "# Change parameters of the model\n",
    "loaded_model_multiple_signatures.calibrate(tf.constant(3.), tf.constant(4.))\n",
    "\n",
    "# Evaluate the same input again - note the difference in both!\n",
    "print(inference_object_array(tf.constant([3., 4., 5.])))\n",
    "print(inference_object_single(tf.constant(5.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this also means that our model is mutable even during inference. In fact the SavedModel format can be used for inference as well as retraining or transfer learning cases.<br>\n",
    "Now we know almost everythin there is to know about the SavedModel format. Let's just take a quick look inside one of the directories created each time we save a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inside a Saved Model\n",
    "Let us have a look at what is in one of the directories we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assets', 'saved_model.pb', 'variables']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(multiple_signatures_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved model consists of several elements.<br>\n",
    "- `saved_model.pb` contains the model architecture that is used to rebuild the function\n",
    "- The directory `variables` contain one or more data files holding the values of the parameters in the model at the time it was saved. For large models with billions of parameters, these data files can grow large. A `variables.index` file maps the stored parameters to their right spot in the function\n",
    "- The directory `assets` holds additional artefacts needed to recreate the function, but should be empty in our case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Keras Example\n",
    "Now that we are clear on the basics, let us take a look at a more realistic workflow. Defining, saving, and serving a model built with Keras.<br>\n",
    "Our model starts with a bit of data and something to be modelled. I have prepared a small dataset consisting of weather observations from a station that observes temperature, relative humidity, air pressure, and whether it is raining or not. Our task is to build a model that predicts whether it rains or not given the temperature, relative humidity, and air pressure. Our target is not to build an awesome or precise model. It is to train, save, and then serve the model.<br>\n",
    "## Example Data\n",
    "First order of business, let us have a look at the example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example observations\n",
      "      pressure  rain  temperature  humidity\n",
      "1299    1002.1     0          1.8      49.0\n",
      "2736    1008.9     0          7.1      63.0\n",
      "347      989.7     0          5.7      74.0\n",
      "131     1010.9     0          6.0      88.0\n",
      "2994    1014.4     0         10.9      31.0\n",
      "1415     996.9     0          1.0      73.0\n",
      "267      996.8     0          6.9      88.0\n",
      "1678     980.8     1          3.5      96.0\n",
      "2459     999.6     0          9.3      80.0\n",
      "1788     996.7     1          5.6      95.0\n",
      "\n",
      "Statistics\n",
      "          pressure  temperature     humidity\n",
      "count  3151.000000  3151.000000  3151.000000\n",
      "mean   1005.235989     4.737417    71.054268\n",
      "std      15.113994     3.998234    20.464912\n",
      "min     951.100000    -6.400000    22.000000\n",
      "25%     996.150000     2.200000    55.000000\n",
      "50%    1005.700000     5.000000    75.000000\n",
      "75%    1016.500000     6.700000    89.000000\n",
      "max    1036.400000    20.100000    97.000000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/data.csv')\n",
    "feature_cols = ['pressure', 'temperature', 'humidity']\n",
    "label_col = ['rain']\n",
    "print(\"Example observations\")\n",
    "print(df.sample(10))\n",
    "print()\n",
    "print(\"Statistics\")\n",
    "print(df[feature_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is of relatively high quality, all we need to do is to reduce the numerical difference and align the variances. Specifically, we will standardise (z norm) the features. If this CONTINUE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZNorm(tf.Module):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, num_features):\n",
    "        super(ZNorm, self).__init__()\n",
    "        self.bias = tf.Variable(1.)\n",
    "        self.weight = tf.Variable(2.)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        Linearly rescale y = x * weight + bias\n",
    "        :param x: The variable to be linearly scaled\n",
    "        :type x: tf.float32\n",
    "        :output dict: \"y\" = x * weight + bias\n",
    "        '''\n",
    "        \n",
    "        return {\"y\" : x * self.weight + self.bias}\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None], tf.float32), tf.TensorSpec([None], tf.float32)])\n",
    "    def calibrate(self, means, standard_deviations):\n",
    "        '''\n",
    "        Set the parameters of the linear function.\n",
    "        :param weight: Weight of the linear function\n",
    "        :type weight: tf.float32\n",
    "        :param bias: Bias of the linear function\n",
    "        :type bias: tf.float32\n",
    "        '''\n",
    "        self.weight.assign(weight)\n",
    "        self.bias.assign(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_cols].values\n",
    "y = df[label_col].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymodel(X,y):\n",
    "\n",
    "    tf.random.set_seed(seed=0)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True)\n",
    "    \n",
    "    # Create Keras model\n",
    "    model = keras.Sequential(layers=[\n",
    "        keras.layers.InputLayer(input_shape=(3), name=\"input\"),\n",
    "        keras.layers.Dense(3, activation=\"relu\", name=\"dense\"),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")\n",
    "    ],\n",
    "                             name=\"mymodel\")\n",
    "\n",
    "    # Print model architecture\n",
    "    model.summary()\n",
    "\n",
    "    # Compile model with optimizer\n",
    "    model.compile(optimizer=keras.optimizers.Adam(0.001),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train model\n",
    "    model.fit(x=[X_train], y=[y_train], batch_size=100, epochs=10)\n",
    "\n",
    "    # Test model\n",
    "    test_loss, test_acc = model.evaluate(x=[X_test],\n",
    "                                         y=[y_test],\n",
    "                                         verbose=2)\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Test accuracy: \")\n",
    "    print(test_acc)\n",
    "\n",
    "    # Get predictions for test images\n",
    "    predictions = model.predict(X_test)\n",
    "    # Print the prediction for the first image\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Example prediction reference:\")\n",
    "    print(predictions)\n",
    "\n",
    "    # Save model to SavedModel format\n",
    "    tf.saved_model.save(model, \"./models\")\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
